{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "426447fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_tweedie_deviance\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, RFE\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "import ssl\n",
    "import urllib.request\n",
    "\n",
    "# Create unverified SSL context to handle certificate issues\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cda1b80",
   "metadata": {},
   "source": [
    "The cell below outputs the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f3e5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_load\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Data\n\u001b[0;32m      2\u001b[0m load \u001b[38;5;241m=\u001b[39m Data()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHOMES_FOR_SALE\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m load\u001b[38;5;241m.\u001b[39mINCOME()\n\u001b[0;32m      5\u001b[0m load\u001b[38;5;241m.\u001b[39mHOMICIDES()\n",
      "File \u001b[1;32mc:\\Users\\Micha\\OneDrive\\Documents\\OMSA\\Semester 4\\CDA\\Final Proj\\Github\\House_Prices_Project\\data_load.py:17\u001b[0m, in \u001b[0;36mData.HOMES_FOR_SALE\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mHOMES_FOR_SALE\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[43mkagglehub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_download\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhouses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(path):\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\kagglehub\\datasets.py:43\u001b[0m, in \u001b[0;36mdataset_download\u001b[1;34m(handle, path, force_download)\u001b[0m\n\u001b[0;32m     41\u001b[0m h \u001b[38;5;241m=\u001b[39m parse_dataset_handle(handle)\n\u001b[0;32m     42\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;241m.\u001b[39mto_url()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m, extra\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mEXTRA_CONSOLE_BLOCK})\n\u001b[1;32m---> 43\u001b[0m path, _ \u001b[38;5;241m=\u001b[39m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\kagglehub\\registry.py:28\u001b[0m, in \u001b[0;36mMultiImplRegistry.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impls):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mis_supported(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m         fails\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtype\u001b[39m(impl)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\kagglehub\\resolver.py:29\u001b[0m, in \u001b[0;36mResolver.__call__\u001b[1;34m(self, handle, path, force_download)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m, handle: T, path: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, force_download: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resolves a handle into a path with the requested file(s) and the resource's version number.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m        Some cases where version number might be missing: Competition datasource, API-based models.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     path, version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     register_datasource_access(handle, version)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\kagglehub\\http_resolver.py:107\u001b[0m, in \u001b[0;36mDatasetHttpResolver._resolve\u001b[1;34m(self, h, path, force_download)\u001b[0m\n\u001b[0;32m    104\u001b[0m api_client \u001b[38;5;241m=\u001b[39m KaggleApiV1Client()\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m h\u001b[38;5;241m.\u001b[39mis_versioned():\n\u001b[1;32m--> 107\u001b[0m     h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mwith_version(\u001b[43m_get_current_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    109\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m load_from_cache(h, path)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_path \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force_download:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\kagglehub\\http_resolver.py:290\u001b[0m, in \u001b[0;36m_get_current_version\u001b[1;34m(api_client, h)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_response[MODEL_INSTANCE_VERSION_FIELD]\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(h, DatasetHandle):\n\u001b[1;32m--> 290\u001b[0m     json_response \u001b[38;5;241m=\u001b[39m \u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_build_get_dataset_url_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DATASET_CURRENT_VERSION_FIELD \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m json_response:\n\u001b[0;32m    292\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid GetDataset API response. Expected to include a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_CURRENT_VERSION_FIELD\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m field\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\kagglehub\\clients.py:135\u001b[0m, in \u001b[0;36mKaggleApiV1Client.get\u001b[1;34m(self, path, resource_handle)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, resource_handle: Optional[ResourceHandle] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m    132\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_url(path)\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    134\u001b[0m         url,\n\u001b[1;32m--> 135\u001b[0m         headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mget_user_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m},\n\u001b[0;32m    136\u001b[0m         auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_auth(),\n\u001b[0;32m    137\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m(DEFAULT_CONNECT_TIMEOUT, DEFAULT_READ_TIMEOUT),\n\u001b[0;32m    138\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m    139\u001b[0m         kaggle_api_raise_for_status(response, resource_handle)\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_for_version_update(response)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\kagglehub\\clients.py:78\u001b[0m, in \u001b[0;36mget_user_agent\u001b[1;34m()\u001b[0m\n\u001b[0;32m     76\u001b[0m user_agents \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkagglehub/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkagglehub\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lib \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras_hub\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras_nlp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras_cv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtune\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 78\u001b[0m     lib_info \u001b[38;5;241m=\u001b[39m \u001b[43msearch_lib_in_call_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lib_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m         user_agents\u001b[38;5;241m.\u001b[39mappend(lib_info)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\kagglehub\\env.py:63\u001b[0m, in \u001b[0;36msearch_lib_in_call_stack\u001b[1;34m(lib_name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_lib_in_call_stack\u001b[39m(lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search the call stack for a given library name and get its information.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m        str: A formatted string f\"{lib_name}/{lib_version}\" if found, otherwise None.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m frame_info \u001b[38;5;129;01min\u001b[39;00m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     64\u001b[0m         module \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(frame_info\u001b[38;5;241m.\u001b[39mframe)\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\inspect.py:1751\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(context)\u001b[0m\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack\u001b[39m(context\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetouterframes\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getframe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\inspect.py:1726\u001b[0m, in \u001b[0;36mgetouterframes\u001b[1;34m(frame, context)\u001b[0m\n\u001b[0;32m   1724\u001b[0m framelist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m frame:\n\u001b[1;32m-> 1726\u001b[0m     traceback_info \u001b[38;5;241m=\u001b[39m \u001b[43mgetframeinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1727\u001b[0m     frameinfo \u001b[38;5;241m=\u001b[39m (frame,) \u001b[38;5;241m+\u001b[39m traceback_info\n\u001b[0;32m   1728\u001b[0m     framelist\u001b[38;5;241m.\u001b[39mappend(FrameInfo(\u001b[38;5;241m*\u001b[39mframeinfo, positions\u001b[38;5;241m=\u001b[39mtraceback_info\u001b[38;5;241m.\u001b[39mpositions))\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\inspect.py:1688\u001b[0m, in \u001b[0;36mgetframeinfo\u001b[1;34m(frame, context)\u001b[0m\n\u001b[0;32m   1686\u001b[0m start \u001b[38;5;241m=\u001b[39m lineno \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m context\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m     lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1690\u001b[0m     lines \u001b[38;5;241m=\u001b[39m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\inspect.py:1071\u001b[0m, in \u001b[0;36mfindsource\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m   1068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (file\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m   1069\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource code not available\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1071\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mgetmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module:\n\u001b[0;32m   1073\u001b[0m     lines \u001b[38;5;241m=\u001b[39m linecache\u001b[38;5;241m.\u001b[39mgetlines(file, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\inspect.py:997\u001b[0m, in \u001b[0;36mgetmodule\u001b[1;34m(object, _filename)\u001b[0m\n\u001b[0;32m    994\u001b[0m         f \u001b[38;5;241m=\u001b[39m getabsfile(module)\n\u001b[0;32m    995\u001b[0m         \u001b[38;5;66;03m# Always map to the name the module knows itself by\u001b[39;00m\n\u001b[0;32m    996\u001b[0m         modulesbyfile[f] \u001b[38;5;241m=\u001b[39m modulesbyfile[\n\u001b[1;32m--> 997\u001b[0m             \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m modulesbyfile:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mget(modulesbyfile[file])\n",
      "File \u001b[1;32m<frozen ntpath>:696\u001b[0m, in \u001b[0;36mrealpath\u001b[1;34m(path, strict)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from data_load import Data\n",
    "load = Data()\n",
    "load.HOMES_FOR_SALE()\n",
    "load.INCOME()\n",
    "load.HOMICIDES()\n",
    "load.POPULATION()\n",
    "load.ZIP_COUNTY()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc3c056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Micha\\OneDrive\\Documents\\OMSA\\Semester 4\\CDA\\Final Proj\\Github\\House_Prices_Project\\Final_Data_Output.py:92: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Count'] = df['Count'].fillna(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1083406, 55)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Final_Data_Output import Final_Data as FD\n",
    "FD = FD()\n",
    "Master_df = FD.Merge_all(min_price=1000, max_bed=12, max_bath=10)\n",
    "Master_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4609610",
   "metadata": {},
   "source": [
    "The below function is new and for prepping all features in the dataframe for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58249779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total available features: 30\n",
      "Available features: ['bed', 'bath', 'house_size', 'zip_code', 'acre_lot', 'Household_AGI', 'Total_Pop', 'price_to_income_ratio', 'price_per_sqft', 'total_rooms', 'bed_bath_ratio', 'space_per_room', 'economic_health', 'population_density', 'lot_to_house_ratio', 'affordability_numeric', 'value_numeric', 'safety_numeric', 'recommendation_score', 'log_income', 'log_house_size', 'affordability_score_encoded', 'value_tier_encoded', 'family_suitability_encoded', 'area_prosperity_encoded', 'safety_score_encoded', 'lifestyle_type_encoded', 'expansion_potential_encoded', 'buyer_profile_encoded', 'state_market_encoded']\n"
     ]
    }
   ],
   "source": [
    "# Add this cell after loading Master_df but before train_test_split\n",
    "def prepare_features_for_selection(df):\n",
    "    \"\"\"Prepare all features including engineered ones for selection\"\"\"\n",
    "    \n",
    "    # Handle categorical variables with label encoding\n",
    "    le = LabelEncoder()\n",
    "    categorical_cols = ['affordability_score', 'value_tier', 'family_suitability', \n",
    "                       'area_prosperity', 'safety_score', 'lifestyle_type', \n",
    "                       'expansion_potential', 'buyer_profile', 'state_market']\n",
    "    \n",
    "    df_encoded = df.copy()\n",
    "    for col in categorical_cols:\n",
    "        if col in df_encoded.columns:\n",
    "            df_encoded[col + '_encoded'] = le.fit_transform(df_encoded[col].astype(str))\n",
    "    \n",
    "    # Define all potential features\n",
    "    all_features = [\n",
    "        # Original features\n",
    "        'bed', 'bath', 'house_size', 'zip_code', 'acre_lot', 'Household_AGI', 'Total_Pop',\n",
    "        \n",
    "        # Engineered numeric features\n",
    "        'price_to_income_ratio', 'price_per_sqft', 'total_rooms', 'bed_bath_ratio',\n",
    "        'space_per_room', 'economic_health', 'population_density', 'lot_to_house_ratio',\n",
    "        'affordability_numeric', 'value_numeric', 'safety_numeric', 'recommendation_score',\n",
    "        'log_income', 'log_house_size',\n",
    "        \n",
    "        # Encoded categorical features\n",
    "        'affordability_score_encoded', 'value_tier_encoded', 'family_suitability_encoded',\n",
    "        'area_prosperity_encoded', 'safety_score_encoded', 'lifestyle_type_encoded',\n",
    "        'expansion_potential_encoded', 'buyer_profile_encoded', 'state_market_encoded'\n",
    "    ]\n",
    "    \n",
    "    # Filter to only include features that exist in the dataframe\n",
    "    available_features = [f for f in all_features if f in df_encoded.columns]\n",
    "    \n",
    "    return df_encoded, available_features\n",
    "\n",
    "# Prepare the data\n",
    "Master_df_encoded, all_feature_columns = prepare_features_for_selection(Master_df)\n",
    "Master_df_encoded.dropna(axis=0, inplace=True)\n",
    "\n",
    "print(f\"Total available features: {len(all_feature_columns)}\")\n",
    "print(\"Available features:\", all_feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4589d2",
   "metadata": {},
   "source": [
    "The below function is new and for analyzing the missing data each of the csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff42876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE Cell #6 - Fix the missing data analysis function\n",
    "def analyze_missing_data(df_name, df):\n",
    "    \"\"\"Comprehensive missing data analysis\"\"\"\n",
    "    print(f\"\\n=== MISSING DATA ANALYSIS: {df_name} ===\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Count missing values\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_percent = (missing_counts / len(df)) * 100\n",
    "    \n",
    "    # Create summary\n",
    "    missing_summary = pd.DataFrame({  # FIX: Use consistent variable name\n",
    "        'Column': missing_counts.index,\n",
    "        'Missing_Count': missing_counts.values,\n",
    "        'Missing_Percent': missing_percent.values\n",
    "    })\n",
    "    \n",
    "    # Filter only columns with missing data - FIXED\n",
    "    missing_summary = missing_summary[missing_summary['Missing_Count'] > 0].sort_values('Missing_Percent', ascending=False)\n",
    "    \n",
    "    if len(missing_summary) > 0:\n",
    "        print(\"Columns with missing data:\")\n",
    "        print(missing_summary.to_string(index=False))\n",
    "        \n",
    "        # Show data types for missing columns\n",
    "        print(\"\\nData types of columns with missing data:\")\n",
    "        for col in missing_summary['Column']:\n",
    "            print(f\"{col}: {df[col].dtype}\")\n",
    "            \n",
    "        return missing_summary\n",
    "    else:\n",
    "        print(\"✅ No missing values found!\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f1ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MISSING DATA ANALYSIS: Houses_Sold ===\n",
      "Dataset shape: (2226382, 13)\n",
      "Columns with missing data:\n",
      "        Column  Missing_Count  Missing_Percent\n",
      "prev_sold_date         734297        32.981627\n",
      "    house_size         568484        25.533983\n",
      "          bath         511771        22.986666\n",
      "           bed         481317        21.618797\n",
      "      acre_lot         325589        14.624130\n",
      "        street          10866         0.488056\n",
      "   brokered_by           4533         0.203604\n",
      "         price           1541         0.069215\n",
      "          city           1407         0.063197\n",
      "      zip_code            299         0.013430\n",
      "         state              8         0.000359\n",
      "\n",
      "Data types of columns with missing data:\n",
      "prev_sold_date: object\n",
      "house_size: float64\n",
      "bath: float64\n",
      "bed: float64\n",
      "acre_lot: float64\n",
      "street: float64\n",
      "brokered_by: float64\n",
      "price: float64\n",
      "city: object\n",
      "zip_code: float64\n",
      "state: object\n"
     ]
    }
   ],
   "source": [
    "# Houses data\n",
    "houses_df = pd.read_csv('Houses_Sold.csv')\n",
    "houses_missing = analyze_missing_data(\"Houses_Sold\", houses_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27726a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MISSING DATA ANALYSIS: Income_ZipCode ===\n",
      "Dataset shape: (166131, 166)\n",
      "✅ No missing values found!\n"
     ]
    }
   ],
   "source": [
    "# Income data\n",
    "income_df = pd.read_csv('Income_ZipCode.csv')\n",
    "income_missing = analyze_missing_data(\"Income_ZipCode\", income_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf7cb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MISSING DATA ANALYSIS: Zip_Pop ===\n",
      "Dataset shape: (33772, 3)\n",
      "✅ No missing values found!\n"
     ]
    }
   ],
   "source": [
    "# Population data\n",
    "pop_df = pd.read_csv('zip_pop.csv')\n",
    "pop_missing = analyze_missing_data(\"Zip_Pop\", pop_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56026236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MISSING DATA ANALYSIS: HHS_homicides ===\n",
      "Dataset shape: (132000, 13)\n",
      "Columns with missing data:\n",
      "        Column  Missing_Count  Missing_Percent\n",
      "TTM_Date_Range         113142        85.713636\n",
      "     Rate_M_CI          61871        46.871970\n",
      "\n",
      "Data types of columns with missing data:\n",
      "TTM_Date_Range: object\n",
      "Rate_M_CI: object\n"
     ]
    }
   ],
   "source": [
    "# Homicides data\n",
    "homicides_df = pd.read_csv('HHS_homicides.csv')\n",
    "homicides_missing = analyze_missing_data(\"HHS_homicides\", homicides_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c19b860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MISSING DATA ANALYSIS: mapping_County ===\n",
      "Dataset shape: (54559, 9)\n",
      "✅ No missing values found!\n"
     ]
    }
   ],
   "source": [
    "# County mapping data\n",
    "county_df = pd.read_csv('mapping_County.csv')\n",
    "county_missing = analyze_missing_data(\"mapping_County\", county_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4362177d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\_function_base_impl.py:2999: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\Micha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\_function_base_impl.py:3000: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correlation Top Features:\n",
      " 1. price_to_income_ratio\n",
      " 2. price_per_sqft\n",
      " 3. Household_AGI\n",
      " 4. bath\n",
      " 5. affordability_numeric\n",
      " 6. log_income\n",
      " 7. value_numeric\n",
      " 8. total_rooms\n",
      " 9. log_house_size\n",
      "10. recommendation_score\n",
      "\n",
      "F_Regression Top Features:\n",
      " 1. price_to_income_ratio\n",
      " 2. price_per_sqft\n",
      " 3. Household_AGI\n",
      " 4. bath\n",
      " 5. log_income\n",
      " 6. value_numeric\n",
      " 7. total_rooms\n",
      " 8. log_house_size\n",
      " 9. recommendation_score\n",
      "10. buyer_profile_encoded\n",
      "\n",
      "Mutual_Info Top Features:\n",
      " 1. price_per_sqft\n",
      " 2. price_to_income_ratio\n",
      " 3. zip_code\n",
      " 4. value_numeric\n",
      " 5. log_income\n",
      " 6. Household_AGI\n",
      " 7. buyer_profile_encoded\n",
      " 8. value_tier_encoded\n",
      " 9. Total_Pop\n",
      "10. recommendation_score\n",
      "\n",
      "Random_Forest Top Features:\n",
      " 1. price_per_sqft\n",
      " 2. house_size\n",
      " 3. log_house_size\n",
      " 4. price_to_income_ratio\n",
      " 5. buyer_profile_encoded\n",
      " 6. space_per_room\n",
      " 7. population_density\n",
      " 8. lot_to_house_ratio\n",
      " 9. bed_bath_ratio\n",
      "10. zip_code\n",
      "\n",
      "RFE_XGBoost Top Features:\n",
      " 1. bed\n",
      " 2. bath\n",
      " 3. house_size\n",
      " 4. acre_lot\n",
      " 5. Household_AGI\n",
      " 6. Total_Pop\n",
      " 7. price_to_income_ratio\n",
      " 8. price_per_sqft\n",
      " 9. bed_bath_ratio\n",
      "10. space_per_room\n"
     ]
    }
   ],
   "source": [
    "# NEW Cell #6 - Add comprehensive feature selection\n",
    "def comprehensive_feature_selection(X, y, feature_names, target_features=15):\n",
    "    \"\"\"Apply multiple feature selection methods and compare results\"\"\"\n",
    "    \n",
    "    feature_importance_methods = {}\n",
    "    \n",
    "    # 1. Correlation with target\n",
    "    correlations = {}\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        correlations[feature] = abs(np.corrcoef(X.iloc[:, i], y)[0, 1])\n",
    "    \n",
    "    # Sort by correlation\n",
    "    correlation_ranking = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "    feature_importance_methods['Correlation'] = [f[0] for f in correlation_ranking[:target_features]]\n",
    "    \n",
    "    # 2. Univariate Statistical Tests\n",
    "    selector_f = SelectKBest(score_func=f_regression, k=target_features)\n",
    "    X_f_selected = selector_f.fit_transform(X, y)\n",
    "    f_scores = selector_f.scores_\n",
    "    f_ranking = [(feature_names[i], f_scores[i]) for i in selector_f.get_support(indices=True)]\n",
    "    f_ranking.sort(key=lambda x: x[1], reverse=True)\n",
    "    feature_importance_methods['F_Regression'] = [f[0] for f in f_ranking]\n",
    "    \n",
    "    # 3. Mutual Information\n",
    "    selector_mi = SelectKBest(score_func=mutual_info_regression, k=target_features)\n",
    "    X_mi_selected = selector_mi.fit_transform(X, y)\n",
    "    mi_scores = selector_mi.scores_\n",
    "    mi_ranking = [(feature_names[i], mi_scores[i]) for i in selector_mi.get_support(indices=True)]\n",
    "    mi_ranking.sort(key=lambda x: x[1], reverse=True)\n",
    "    feature_importance_methods['Mutual_Info'] = [f[0] for f in mi_ranking]\n",
    "    \n",
    "    # 4. Tree-based Feature Importance (Random Forest)\n",
    "    rf_selector = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_selector.fit(X, y)\n",
    "    rf_importance = [(feature_names[i], importance) for i, importance in enumerate(rf_selector.feature_importances_)]\n",
    "    rf_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "    feature_importance_methods['Random_Forest'] = [f[0] for f in rf_importance[:target_features]]\n",
    "    \n",
    "    # 5. Recursive Feature Elimination with XGBoost\n",
    "    xgb_selector = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "    rfe_selector = RFE(xgb_selector, n_features_to_select=target_features)\n",
    "    rfe_selector.fit(X, y)\n",
    "    rfe_features = [feature_names[i] for i in range(len(feature_names)) if rfe_selector.support_[i]]\n",
    "    feature_importance_methods['RFE_XGBoost'] = rfe_features\n",
    "    \n",
    "    return feature_importance_methods\n",
    "\n",
    "# Apply feature selection\n",
    "X_all = Master_df_encoded[all_feature_columns]\n",
    "y = Master_df_encoded['price']\n",
    "\n",
    "feature_methods = comprehensive_feature_selection(X_all, y, all_feature_columns)\n",
    "\n",
    "# Display results\n",
    "for method, features in feature_methods.items():\n",
    "    print(f\"\\n{method} Top Features:\")\n",
    "    for i, feature in enumerate(features[:10], 1):\n",
    "        print(f\"{i:2d}. {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fbe584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPOSED FINAL FEATURE SET:\n",
      "========================================\n",
      " 1. house_size                (Selected by 5/5 methods)\n",
      " 2. bed                       (Selected by 3/5 methods)\n",
      " 3. bath                      (Selected by 4/5 methods)\n",
      " 4. price_per_sqft            (Selected by 5/5 methods)\n",
      " 5. Household_AGI             (Selected by 5/5 methods)\n",
      " 6. price_to_income_ratio     (Selected by 5/5 methods)\n",
      " 7. buyer_profile_encoded     (Selected by 5/5 methods)\n",
      " 8. log_income                (Selected by 4/5 methods)\n",
      " 9. value_numeric             (Selected by 4/5 methods)\n",
      "10. log_house_size            (Selected by 4/5 methods)\n",
      "11. total_rooms               (Selected by 3/5 methods)\n",
      "12. recommendation_score      (Selected by 3/5 methods)\n"
     ]
    }
   ],
   "source": [
    "# NEW Cell #7 - Create feature consensus and final feature set\n",
    "def create_feature_consensus(feature_methods, min_votes=2):\n",
    "    \"\"\"Create consensus features based on how many methods selected them\"\"\"\n",
    "    \n",
    "    feature_votes = {}\n",
    "    \n",
    "    # Count votes for each feature\n",
    "    for method, features in feature_methods.items():\n",
    "        for feature in features:\n",
    "            feature_votes[feature] = feature_votes.get(feature, 0) + 1\n",
    "    \n",
    "    # Filter features by minimum votes\n",
    "    consensus_features = {feature: votes for feature, votes in feature_votes.items() \n",
    "                         if votes >= min_votes}\n",
    "    \n",
    "    # Sort by votes\n",
    "    consensus_ranking = sorted(consensus_features.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return consensus_ranking, feature_votes\n",
    "\n",
    "def propose_final_feature_set(consensus_features, target_size=12):\n",
    "    \"\"\"Propose final feature set based on consensus and domain knowledge\"\"\"\n",
    "    \n",
    "    # Core features (always include these for house price prediction)\n",
    "    core_features = ['house_size', 'bed', 'bath', 'price_per_sqft', 'Household_AGI']\n",
    "    \n",
    "    # High consensus features (4+ votes)\n",
    "    high_consensus = [f[0] for f in consensus_features if f[1] >= 4]\n",
    "    \n",
    "    # Medium consensus features (3 votes)\n",
    "    medium_consensus = [f[0] for f in consensus_features if f[1] == 3]\n",
    "    \n",
    "    # Start with core features that exist\n",
    "    final_features = [f for f in core_features if f in all_feature_columns]\n",
    "    \n",
    "    # Add high consensus features (if not already included)\n",
    "    for feature in high_consensus:\n",
    "        if feature not in final_features:\n",
    "            final_features.append(feature)\n",
    "    \n",
    "    # Add medium consensus features until we reach target size\n",
    "    for feature in medium_consensus:\n",
    "        if len(final_features) >= target_size:\n",
    "            break\n",
    "        if feature not in final_features:\n",
    "            final_features.append(feature)\n",
    "    \n",
    "    return final_features[:target_size]\n",
    "\n",
    "# Create consensus and final feature set\n",
    "consensus_features, all_votes = create_feature_consensus(feature_methods, min_votes=2)\n",
    "final_feature_set = propose_final_feature_set(consensus_features, target_size=12)\n",
    "\n",
    "print(\"PROPOSED FINAL FEATURE SET:\")\n",
    "print(\"=\" * 40)\n",
    "for i, feature in enumerate(final_feature_set, 1):\n",
    "    votes = all_votes.get(feature, 0)\n",
    "    print(f\"{i:2d}. {feature:<25} (Selected by {votes}/5 methods)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6afd70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_tweedie_deviance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master_df.dropna(axis=0, inplace=True)\n",
    "# X = Master_df[['bed', 'bath', 'house_size', 'zip_code', 'acre_lot', 'Household_AGI', 'Total_Pop']]\n",
    "# y = Master_df['price']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# REPLACE Cell #7 - New train/test split with selected features\n",
    "# Use the selected features instead of the original limited set\n",
    "\n",
    "X = Master_df_encoded[final_feature_set]  # Changed from limited feature set\n",
    "y = Master_df_encoded['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training with {len(final_feature_set)} selected features:\")\n",
    "print(final_feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e431479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGReg = xgb_reg = xgb.XGBRegressor(\n",
    "    objective='reg:tweedie',\n",
    "    tweedie_variance_power=1.75, # Choose a value between 1 and 2 for overdispersed data\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "XGReg.fit(X_train, y_train)\n",
    "y_pred = XGReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61713702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Tweedie Deviance: 63.5866\n",
      "Null Model Mean Tweedie Deviance: 560.0368\n",
      "Percent Deviance Explained: 88.6460\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using the Mean Tweedie Deviance\n",
    "tweedie_deviance = mean_tweedie_deviance(y_test, y_pred, power=1.5)\n",
    "print(f\"Mean Tweedie Deviance: {tweedie_deviance:.4f}\")\n",
    "\n",
    "null_tweedie_deviance = mean_tweedie_deviance(y_test, [y_train.mean()]*len(y_test), power=1.5)\n",
    "print(f\"Null Model Mean Tweedie Deviance: {null_tweedie_deviance:.4f}\")\n",
    "print(f\"Percent Deviance Explained: {(1-tweedie_deviance/null_tweedie_deviance)*100:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724cd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e441c30",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 5. Random Forest\u001b[39;00m\n\u001b[0;32m     22\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m rf_pred \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 6. Gradient Boosting\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:486\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    475\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    478\u001b[0m ]\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 486\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     73\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[0;32m     74\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     75\u001b[0m     (\n\u001b[0;32m     76\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     81\u001b[0m )\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1986\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1984\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1985\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1988\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1989\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1990\u001b[0m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1991\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1992\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1993\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1914\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1914\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1916\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig), warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[0;32m    146\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilters \u001b[38;5;241m=\u001b[39m warning_filters\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    197\u001b[0m         X,\n\u001b[0;32m    198\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    201\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    202\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "\n",
    "# 2. Ridge Regression (handles multicollinearity)\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge_pred = ridge.predict(X_test)\n",
    "\n",
    "# 3. Lasso Regression (feature selection)\n",
    "lasso = Lasso(alpha=1.0)\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso_pred = lasso.predict(X_test)\n",
    "\n",
    "# 4. Elastic Net (combines Ridge + Lasso)\n",
    "elastic = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "elastic.fit(X_train, y_train)\n",
    "elastic_pred = elastic.predict(X_test)\n",
    "\n",
    "# 5. Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "# 6. Gradient Boosting\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "gb_pred = gb.predict(X_test)\n",
    "\n",
    "# 7. LightGBM (fast gradient boosting)\n",
    "lgb_reg = lgb.LGBMRegressor(n_estimators=100, random_state=42)\n",
    "lgb_reg.fit(X_train, y_train)\n",
    "lgb_pred = lgb_reg.predict(X_test)\n",
    "\n",
    "# 8. Support Vector Regression\n",
    "svr = SVR(kernel='rbf', C=1000, gamma='scale')\n",
    "svr.fit(X_train, y_train)\n",
    "svr_pred = svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models_dict, X_test, y_test, y_train):\n",
    "    \"\"\"Evaluate multiple models and return comparison metrics\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, predictions in models_dict.items():\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        tweedie = mean_tweedie_deviance(y_test, predictions, power=1.5)\n",
    "        \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'MSE': mse,\n",
    "            'MAE': mae,\n",
    "            'R²': r2,\n",
    "            'Tweedie_Deviance': tweedie\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values('R²', ascending=False)\n",
    "\n",
    "# Compare all models\n",
    "models_predictions = {\n",
    "    'XGBoost': y_pred,\n",
    "    'Linear Regression': lr_pred,\n",
    "    'Ridge': ridge_pred,\n",
    "    'Lasso': lasso_pred,\n",
    "    'Elastic Net': elastic_pred,\n",
    "    'Random Forest': rf_pred,\n",
    "    'Gradient Boosting': gb_pred,\n",
    "    'LightGBM': lgb_pred,\n",
    "    'SVR': svr_pred\n",
    "}\n",
    "\n",
    "comparison_df = evaluate_models(models_predictions, X_test, y_test, y_train)\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d43953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Neural Network (MLPRegressor)\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale features for neural network\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "mlp_pred = mlp.predict(X_test_scaled)\n",
    "\n",
    "# 10. Polynomial Features + Linear Regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "poly_lr = LinearRegression()\n",
    "poly_lr.fit(X_train_poly, y_train)\n",
    "poly_pred = poly_lr.predict(X_test_poly)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
